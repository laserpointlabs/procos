# ProcOS Configuration Template
# Copy this file to .env and customize for your environment

# =============================================================================
# PROCOS CORE CONFIGURATION
# =============================================================================

# Environment (development, staging, production)
PROCOS_ENV=development

# Logging configuration
LOG_LEVEL=INFO
LOG_FORMAT=%(asctime)s - ProcOS - %(levelname)s - %(message)s

# System identification
PROCOS_INSTANCE_ID=procos-dev-001
PROCOS_VERSION=1.0.0

# =============================================================================
# CAMUNDA CONFIGURATION
# =============================================================================

# Camunda engine connection
CAMUNDA_BASE_URL=http://localhost:8080
CAMUNDA_ENGINE_NAME=default
CAMUNDA_API_PREFIX=/engine-rest

# External task worker configuration
WORKER_MAX_TASKS=10
WORKER_LOCK_DURATION=300000
WORKER_RETRY_TIMEOUT=30000

# Process deployment
PROCESS_DEPLOYMENT_PATH=./src/processes
AUTO_DEPLOY_PROCESSES=true

# =============================================================================
# MESSAGE BROKER CONFIGURATION
# =============================================================================

# RabbitMQ settings
RABBITMQ_HOST=localhost
RABBITMQ_PORT=5672
RABBITMQ_USERNAME=procos
RABBITMQ_PASSWORD=procos123
RABBITMQ_VIRTUAL_HOST=/

# Queue configuration
RABBITMQ_EXCHANGE=procos.events
RABBITMQ_ROUTING_KEY_PREFIX=procos

# =============================================================================
# REDIS CONFIGURATION
# =============================================================================

# Redis connection
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=

# Cache settings
CACHE_TTL=3600
CACHE_PREFIX=procos:

# =============================================================================
# AI/LLM INTEGRATION
# =============================================================================

# OpenAI configuration
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_ORG_ID=your_organization_id
OPENAI_DEFAULT_MODEL=gpt-4
OPENAI_MAX_TOKENS=2048
OPENAI_TEMPERATURE=0.7

# Ollama configuration (local LLM)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_DEFAULT_MODEL=llama2
OLLAMA_TIMEOUT=60

# AI Worker settings
AI_WORKER_ENABLED=true
AI_WORKER_MAX_CONCURRENT=5
AI_WORKER_TIMEOUT=120